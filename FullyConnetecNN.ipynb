{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from dataset import get_2D_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceLR:\n",
    "    def __init__(self, lr=0.01, patience=10, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.lr = lr\n",
    "        self.best_loss = None\n",
    "        self.reduce_lr = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.reduce_lr = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "            \n",
    "    def reset(self):\n",
    "        self.lr /= 10\n",
    "        self.reduce_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def relu(self, inputs):\n",
    "        \"\"\"\n",
    "        ReLU Activation Function\n",
    "        \"\"\"\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        \"\"\"\n",
    "        Softmax Activation Function\n",
    "        \"\"\"\n",
    "        \n",
    "        in_mean = np.mean(inputs, axis=(0, 1), dtype=np.float16)\n",
    "        in_std = np.std(inputs, axis=(0, 1), dtype=np.float16)\n",
    "        \n",
    "        in_norm = (inputs - in_mean) / in_std\n",
    "        \n",
    "        exp_scores = np.exp(in_norm)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def relu_derivative(self, dA, Z):\n",
    "        \"\"\"\n",
    "        ReLU Derivative Function\n",
    "        \"\"\"\n",
    "        dZ = np.array(dA, copy = True, dtype=np.float16)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    \n",
    "    def forward(self, inputs, weights, bias, activation):\n",
    "        \"\"\"\n",
    "        Single Layer Forward Propagation\n",
    "        \"\"\"\n",
    "        Z_curr = np.dot(inputs, weights.T) + bias\n",
    "        if activation == 'relu':\n",
    "            A_curr = self.relu(inputs=Z_curr)\n",
    "        elif activation == 'softmax':\n",
    "            A_curr = self.softmax(inputs=Z_curr)\n",
    "\n",
    "        return A_curr, Z_curr\n",
    "    \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
    "        \"\"\"\n",
    "        Single Layer Backward Propagation\n",
    "        \"\"\"\n",
    "        if activation == 'softmax':\n",
    "            dW = np.dot(A_prev.T, dA_curr)\n",
    "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
    "            dA = np.dot(dA_curr, W_curr) \n",
    "        else:\n",
    "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
    "            dW = np.dot(A_prev.T, dZ)\n",
    "            db = np.sum(dZ, axis=0, keepdims=True)\n",
    "            dA = np.dot(dZ, W_curr)\n",
    "\n",
    "        return dA, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.network = []  # layers\n",
    "        self.architecture = []  # mapping input neurons --> output neurons\n",
    "        self.params = []  # W, b\n",
    "        self.memory = []  # Z, A\n",
    "        self.gradients = []  # dW, db\n",
    "        self.layers_size = len(self.network)\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Add layers to the network\n",
    "        \"\"\"\n",
    "        self.network.append(layer)\n",
    "        self.layers_size += 1\n",
    "\n",
    "    def _compile(self, data):\n",
    "        \"\"\"\n",
    "        Initialize model architecture\n",
    "        \"\"\"\n",
    "        for idx in range(self.layers_size):\n",
    "\n",
    "            input_dim = data.shape[1] if idx == 0 else self.network[idx - 1].neurons\n",
    "            output_dim = self.network[idx].neurons\n",
    "            activation = 'relu' if idx != self.layers_size - 1 else 'softmax'\n",
    "\n",
    "            self.architecture.append({'input_dim': input_dim,\n",
    "                                      'output_dim': output_dim,\n",
    "                                      'activation': activation})\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _init_weights(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the model parameters \n",
    "        \"\"\"\n",
    "        self._compile(data)\n",
    "\n",
    "        np.random.seed(99)\n",
    "\n",
    "        for i in range(self.layers_size):\n",
    "            w = np.float16(np.random.uniform(low=-1, high=1,\n",
    "                                             size=(self.architecture[i]['output_dim'],\n",
    "                                                   self.architecture[i]['input_dim'])))\n",
    "            b = np.float16(np.zeros((1, self.architecture[i]['output_dim'])))\n",
    "\n",
    "            self.params.append({'W': w, 'b': b})\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _forwardprop(self, data):\n",
    "        \"\"\"\n",
    "        Performs one full forward pass through network\n",
    "        \"\"\"\n",
    "        A_curr = data\n",
    "\n",
    "        for i in range(self.layers_size):\n",
    "            A_prev = A_curr\n",
    "            A_curr, Z_curr = self.network[i].forward(inputs=A_prev,\n",
    "                                                     weights=self.params[i]['W'],\n",
    "                                                     bias=self.params[i]['b'],\n",
    "                                                     activation=self.architecture[i]['activation'])\n",
    "\n",
    "            self.memory.append({'inputs': A_prev, 'Z': Z_curr})\n",
    "\n",
    "        return A_curr\n",
    "\n",
    "    def _backprop(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Performs one full backward pass through network\n",
    "        \"\"\"\n",
    "        num_samples = len(actual)\n",
    "\n",
    "        # compute the gradient on predictions\n",
    "        dscores = predicted\n",
    "        dscores[np.arange(num_samples).reshape(-1, 1), actual] -= 1\n",
    "        dscores /= num_samples\n",
    "\n",
    "        dA_prev = dscores\n",
    "\n",
    "        for idx, layer in reversed(list(enumerate(self.network))):\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            A_prev = self.memory[idx]['inputs']\n",
    "            Z_curr = self.memory[idx]['Z']\n",
    "            W_curr = self.params[idx]['W']\n",
    "\n",
    "            activation = self.architecture[idx]['activation']\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = layer.backward(\n",
    "                dA_curr, W_curr, Z_curr, A_prev, activation)\n",
    "\n",
    "            self.gradients.append({'dW': dW_curr, 'db': db_curr})\n",
    "\n",
    "    def _update(self, lr_reduce):\n",
    "        \"\"\"\n",
    "        Update the model parameters --> lr * gradient\n",
    "        \"\"\"\n",
    "        lr = lr_reduce.get_lr()\n",
    "\n",
    "        reversed_gradients = list(reversed(self.gradients))\n",
    "        for idx in range(self.layers_size):\n",
    "            self.params[idx]['W'] -= lr * reversed_gradients[idx]['dW'].T\n",
    "            self.params[idx]['b'] -= lr * reversed_gradients[idx]['db']\n",
    "\n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Calculate accuracy after each iteration\n",
    "        \"\"\"\n",
    "        return np.mean(np.argmax(predicted, axis=1) == np.argmax(actual, axis=1))\n",
    "        # return np.mean(np.argmax(predicted, axis=1) == actual)\n",
    "\n",
    "    def _calculate_loss(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Calculate cross-entropy loss after each iteration\n",
    "        \"\"\"\n",
    "        samples = len(actual)\n",
    "\n",
    "        correct_logprobs = - \\\n",
    "            np.log(predicted[np.arange(samples).reshape(-1, 1), actual])\n",
    "        data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def train(self, X_train, y_train, epochs):\n",
    "        \"\"\"\n",
    "        Train the model using SGD\n",
    "        \"\"\"\n",
    "\n",
    "        earlyStop = EarlyStopping()\n",
    "        lr_reduce = ReduceLR(lr=0.01, patience=5, delta=1e-4)\n",
    "\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "        self._init_weights(X_train)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            start_time = time.time()\n",
    "            yhat = self._forwardprop(X_train)\n",
    "\n",
    "            self.accuracy.append(self._get_accuracy(\n",
    "                predicted=yhat, actual=y_train))\n",
    "            self.loss.append(self._calculate_loss(\n",
    "                predicted=yhat, actual=y_train))\n",
    "\n",
    "            self._backprop(predicted=yhat, actual=y_train)\n",
    "\n",
    "            earlyStop(self.loss[-1])\n",
    "            lr_reduce(self.loss[-1])\n",
    "\n",
    "            self._update(lr_reduce)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"EPOCH: {i}, ACCURACY: {self.accuracy[-1]}, LOSS: {self.loss[-1]}, TIME: {end_time-start_time} sec\")\n",
    "\n",
    "            if lr_reduce.reduce_lr and not earlyStop.early_stop:\n",
    "                lr_reduce.reset()\n",
    "\n",
    "            if earlyStop.early_stop:\n",
    "                print(f\"Stopping early at epoch: {i}\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = get_2D_normalised()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoded vectors\n",
    "num_classes = 10\n",
    "train_labels_enc = np.eye(num_classes)[train_labels.ravel()]\n",
    "test_labels_enc = np.eye(num_classes)[test_labels.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation sets\n",
    "val_split = 0.2\n",
    "num_examples = train_data.shape[0]\n",
    "val_size = int(val_split * num_examples)\n",
    "train_size = num_examples - val_size\n",
    "\n",
    "x_train = train_data[:train_size].astype('float16')\n",
    "y_train = train_labels_enc[:train_size].astype('int')\n",
    "x_val = train_data[train_size:].astype('float16')\n",
    "y_val = test_labels_enc[train_size:].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model.add(DenseLayer(2048))\n",
    "model.add(DenseLayer(1024))\n",
    "model.add(DenseLayer(512))\n",
    "model.add(DenseLayer(256))\n",
    "model.add(DenseLayer(128))\n",
    "model.add(DenseLayer(64))\n",
    "model.add(DenseLayer(y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_dim': 3072, 'output_dim': 2048, 'activation': 'relu'}, {'input_dim': 2048, 'output_dim': 1024, 'activation': 'relu'}, {'input_dim': 1024, 'output_dim': 512, 'activation': 'relu'}, {'input_dim': 512, 'output_dim': 256, 'activation': 'relu'}, {'input_dim': 256, 'output_dim': 128, 'activation': 'relu'}, {'input_dim': 128, 'output_dim': 64, 'activation': 'relu'}, {'input_dim': 64, 'output_dim': 10, 'activation': 'softmax'}]\n"
     ]
    }
   ],
   "source": [
    "model._compile(x_train)\n",
    "print(model.architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
